{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing dependencies\n",
    "\n",
    "The following lines assume you're running this script using python version 3.6, on a Linux machine with CUDA GPU.\n",
    "If any aforementioned assumption is incorrect, please replace the first pip3 install line in the next box with the correct requirement from https://pytorch.org/get-started/locally/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies: done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Installing dependencies...\")\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "!{sys.executable} -m pip install torch==1.10.2+cu113 torchvision==0.11.3+cu113 torchaudio==0.10.2+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html --ignore-installed\n",
    "!{sys.executable} -m pip install numpy pandas matplotlib tqdm simpletransformers  --ignore-installed\n",
    "!{sys.executable} -m pip install -U scikit-learn --ignore-installed\n",
    "!{sys.executable} -m pip install simpletransformers --ignore-installed\n",
    "clear_output()\n",
    "\n",
    "print(\"Installing dependencies: done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages & Setting up CUDA GPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing installed packages...\n",
      "device= cuda\n",
      "Emptying CUDA GPU Cache\n",
      "Importing installed packages: done\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing installed packages...\")\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "#%%\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device=\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Emptying CUDA GPU Cache\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Importing installed packages: done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract datasets from csvs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets from csvs...\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting datasets from csvs...\")\n",
    "train_ids = pd.read_csv('./datasets/train_semeval_parids-labels.csv')\n",
    "test_ids = pd.read_csv('./datasets/dev_semeval_parids-labels.csv')\n",
    "data_pcl = pd.read_csv(\"./datasets/dontpatronizeme_pcl.tsv\", sep=\"\\t\", skiprows=3,\n",
    "                        names=['par_id','art_id','keyword','country_code','text','label'])\n",
    "\n",
    "# Introducing yes/no PCL label from PCL score annotation\n",
    "data_pcl['is_pcl'] = data_pcl.label >= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Pre-processing dataset\")\n",
    "# Seperate train and test df according to train_ids and test_ids\n",
    "train_df = data_pcl.loc[data_pcl.par_id.isin(train_ids.par_id)][['par_id','keyword','text', 'is_pcl']]\n",
    "test_df = data_pcl.loc[data_pcl.par_id.isin(test_ids.par_id)][['par_id','text', 'is_pcl']]\n",
    "\n",
    "yes_pcl = train_df.loc[train_df.is_pcl==True]\n",
    "no_pcl = train_df.loc[train_df.is_pcl==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data frame into train and validation sets\n",
    "def separate_train_validation(pcl_df, percent_validation=0.10):\n",
    "    assert(0 <= percent_validation <= 1)\n",
    "    keywords = set(train_df.keyword.to_list())\n",
    "    pcl_validation_list = []\n",
    "    pcl_train_list = []\n",
    "    for keyword in keywords:\n",
    "        pcl_with_keyword = pcl_df.loc[pcl_df.keyword==keyword]\n",
    "        validation_set_len = int(np.floor(len(pcl_with_keyword) * percent_validation))\n",
    "        pcl_validation_list.append(pcl_with_keyword[:validation_set_len])\n",
    "        pcl_train_list.append(pcl_with_keyword[validation_set_len:])\n",
    "\n",
    "    pcl_validation_df = pd.concat(pcl_validation_list)\n",
    "    pcl_train_df = pd.concat(pcl_train_list)\n",
    "\n",
    "    return pcl_validation_df, pcl_train_df \n",
    "\n",
    "yes_pcl_validation, yes_pcl_train = separate_train_validation(yes_pcl)\n",
    "no_pcl_validation, no_pcl_train = separate_train_validation(no_pcl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting training dataframe with translations\n",
      "nb yes 6471\n",
      "nb no 6827\n",
      "nb yes 675\n",
      "nb no 754\n",
      "Data augmentation: done\n",
      "PCL len(validation) =  675\n",
      "NON-PCL len(validation) =  754\n",
      "PCL len(train) =  6471\n",
      "NON-PCL len(train) =  6827\n"
     ]
    }
   ],
   "source": [
    "# Augment yes_pcl data frame with backtranslated data\n",
    "def augment_with_translations(yes_pcl_df, no_pcl_df):\n",
    "    yes_pcl_translations = pd.read_csv(\"./datasets/data_pcl_translations.csv\")\n",
    "    yes_pcl_translations['is_pcl'] = True\n",
    "    yes_pcl_translations = yes_pcl_translations.loc[yes_pcl_translations.par_id.isin(yes_pcl_df.par_id)]\n",
    "    yes_pcl_df = pd.concat([yes_pcl_df, yes_pcl_translations])\n",
    "    new_df = pd.concat([yes_pcl_df, no_pcl_df])[['text', 'is_pcl']]\n",
    "    print('nb yes', (new_df['is_pcl'] > .5).sum())\n",
    "    print('nb no', (new_df['is_pcl'] < .5).sum())\n",
    "    return new_df\n",
    "\n",
    "print(\"Augmenting training dataframe with translations\")\n",
    "new_train = augment_with_translations(yes_pcl_train, no_pcl_train)\n",
    "new_validation = augment_with_translations(yes_pcl_validation, no_pcl_validation)\n",
    "\n",
    "print(\"Data augmentation: done\")\n",
    "print('PCL len(validation) = ', (new_validation['is_pcl'] > .5).sum())\n",
    "print('NON-PCL len(validation) = ', (new_validation['is_pcl'] < .5).sum())\n",
    "print('PCL len(train) = ', (new_train['is_pcl'] > .5).sum())\n",
    "print('NON-PCL len(train) = ', (new_train['is_pcl'] < .5).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Trained Roberta Model Generated.\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating model...\")\n",
    "# Generate model\n",
    "batch_size = 64\n",
    "seq_length = 128\n",
    "epochs = 25\n",
    "\n",
    "task1_model_args = ClassificationArgs(num_train_epochs=epochs,\n",
    "                                      no_save=False,\n",
    "                                      no_cache=False,\n",
    "                                      overwrite_output_dir=True,\n",
    "                                      evaluate_during_training=True, \n",
    "                                      output_dir=f'./outputs/test_roberta_bs_{batch_size}_seq_{seq_length}', #by default\n",
    "                                      best_model_dir=f'./outputs/test_roberta_bs_{batch_size}_seq_{seq_length}/best_model',\n",
    "                                      max_seq_length=seq_length, #by default 128, it could be intresting to see if this trucates our texts\n",
    "                                      save_eval_checkpoints=False,\n",
    "                                      save_model_every_epoch=True,\n",
    "                                      save_steps=-1,\n",
    "                                      evaluate_during_training_verbose=False,\n",
    "                                      learning_rate=4e-5,\n",
    "                                      train_batch_size=batch_size,\n",
    "                                      early_stopping_metric='f1',\n",
    "                                      early_stopping_metric_minimize=False,\n",
    "                                      early_stopping_patience=100,\n",
    "                                      )\n",
    "\n",
    "\n",
    "task1_model = ClassificationModel(\"roberta\", \"roberta-base\",\n",
    "                                    args=task1_model_args,\n",
    "                                    use_cuda=torch.cuda.is_available()\n",
    "                                    )\n",
    "\n",
    "print(\"Pre-Trained Roberta Model Generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/tg4018/anaconda3/envs/nlp_text/lib/python3.6/site-packages/simpletransformers/classification/classification_model.py:586: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628ac1ee5bc2487b96d0c2b711dcfabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/tg4018/anaconda3/envs/nlp_text/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc18eb79db948f5b8452884720ebd3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c134f1878ed45deb80182c6c40b2772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 1:   0%|          | 0/1663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/tg4018/anaconda3/envs/nlp_text/lib/python3.6/site-packages/simpletransformers/classification/classification_model.py:1427: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26962573bcec4d43997fa6cf4ae296cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1429 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "task1_model.train_model(new_train, show_running_loss=True, eval_df=new_validation, f1=f1_score)\n",
    "print(\"Model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction for dev dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/tg4018/anaconda3/envs/nlp_text/lib/python3.6/site-packages/simpletransformers/classification/classification_model.py:1427: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6399ba5a1bef425f85e869f2add79e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3826e3d4bb247eda7e92f774f35749d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# run predictions\n",
    "\n",
    "test_df = test_df[['text', 'is_pcl']]\n",
    "test_data = test_df['text'].to_list()\n",
    "\n",
    "print(\"Running prediction for dev dataset\")\n",
    "_, preds_task1, _ = task1_model.eval_model(test_df)\n",
    "preds_task1 = preds_task1.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9207258834765998\n",
      "Precision: 0.5932203389830508\n",
      "Recall: 0.3841639374298605\n",
      "F1-score: 0.46633420791993185\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate predictions\n",
    "true_positive = ((preds_task1 == 1) & (test_df.is_pcl == preds_task1)).sum() / (\n",
    "    preds_task1 == 1).sum()\n",
    "false_positive = ((preds_task1 == 1) & (test_df.is_pcl != preds_task1)).sum() / (\n",
    "    preds_task1 == 1).sum()\n",
    "true_negative = ((preds_task1 == 0) & (test_df.is_pcl == preds_task1)).sum() / (\n",
    "    preds_task1 == 0).sum()\n",
    "false_negative = ((preds_task1 == 0) & (test_df.is_pcl != preds_task1)).sum() / (\n",
    "    preds_task1 == 0).sum()\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "recall = true_positive / (true_positive + true_negative)\n",
    "accuracy = (test_df.is_pcl == preds_task1).mean()\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "117772926163acf968ad718be0eeaf333e63baced87c985ce66095acc32ba864"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('nlp_text')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
